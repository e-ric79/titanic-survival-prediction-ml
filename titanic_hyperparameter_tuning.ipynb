{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVwlKcEOHTRF"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploads=files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score,GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "df=pd.read_csv('Titanic-Dataset.csv')\n",
        "#cleaning data\n",
        "df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "df['Fare'] = df['Fare'].fillna(df['Fare'].median())\n",
        "df['Sex'] = df['Sex'].map({'male': 1, 'female': 0})\n",
        "df = df.dropna()\n",
        "df.head()\n",
        "X = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']]\n",
        "y = df['Survived']\n",
        "print(f\"Data ready: {X.shape[0]} passengers\")\n",
        "print(\"Starting hyperparameter tuning...\")"
      ],
      "metadata": {
        "id": "C_hg8249kBRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define hyperparameter grid to test\n",
        "param_grid={\n",
        "    'n_estimators': [50,100,200],  #number of trees\n",
        "    'max_depth': [5,10,15,None],   #tree depth\n",
        "    'min_samples_split': [2,5,10], #min samples to split\n",
        "    'min_samples_leaf': [1,2,4]    #min samples in leaf\n",
        "}\n",
        "#calculate total combinations\n",
        "total_combinations=(len(param_grid['n_estimators'])*\n",
        "                    len(param_grid['max_depth'])*\n",
        "                    len(param_grid['min_samples_leaf'])*\n",
        "                    len(param_grid['min_samples_split']))\n",
        "print(f\"Testing {total_combinations} different combinations...\")\n",
        "print(\"This will take a minute...\\n\")\n",
        "#create base model\n",
        "rf=RandomForestClassifier(random_state=42)\n",
        "#grid search with 5 fold cv\n",
        "grid_search=GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                 #5 fold cross validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=1,             #use all cpu cores\n",
        "    verbose=1             #show progress\n",
        ")\n",
        "#fit\n",
        "grid_search.fit(X,y)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BEST HYPERPARAMETERS FOUND:\")\n",
        "print(\"=\"*60)\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"{param}: {value}\")\n",
        "print(\"\\n\"+\"=\"*60)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Default Random Forest Accuracy: 0.8100\")\n",
        "print(f\"Improvement: {(grid_search.best_score_-0.81)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "rBv3tUsZYsGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all results\n",
        "results = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Plot: n_estimators vs accuracy\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. n_estimators effect\n",
        "for depth in [5, 10, 15, None]:\n",
        "    mask = results['param_max_depth'] == depth\n",
        "    data = results[mask].groupby('param_n_estimators')['mean_test_score'].mean()\n",
        "    axes[0, 0].plot(data.index, data.values, marker='o', label=f'depth={depth}')\n",
        "axes[0, 0].set_xlabel('n_estimators')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].set_title('Effect of Number of Trees')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# 2. max_depth effect\n",
        "depth_scores = results.groupby('param_max_depth')['mean_test_score'].mean().sort_index()\n",
        "axes[0, 1].bar(range(len(depth_scores)), depth_scores.values)\n",
        "axes[0, 1].set_xticks(range(len(depth_scores)))\n",
        "axes[0, 1].set_xticklabels(depth_scores.index)\n",
        "axes[0, 1].set_xlabel('max_depth')\n",
        "axes[0, 1].set_ylabel('Accuracy')\n",
        "axes[0, 1].set_title('Effect of Tree Depth')\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# 3. min_samples_split effect\n",
        "split_scores = results.groupby('param_min_samples_split')['mean_test_score'].mean()\n",
        "axes[1, 0].plot(split_scores.index, split_scores.values, marker='o', color='green')\n",
        "axes[1, 0].set_xlabel('min_samples_split')\n",
        "axes[1, 0].set_ylabel('Accuracy')\n",
        "axes[1, 0].set_title('Effect of Min Samples Split')\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# 4. Top 10 configurations\n",
        "top_10 = results.nlargest(10, 'mean_test_score')[['params', 'mean_test_score']]\n",
        "axes[1, 1].barh(range(10), top_10['mean_test_score'].values)\n",
        "axes[1, 1].set_yticks(range(10))\n",
        "axes[1, 1].set_yticklabels([f\"Config {i+1}\" for i in range(10)])\n",
        "axes[1, 1].set_xlabel('Accuracy')\n",
        "axes[1, 1].set_title('Top 10 Configurations')\n",
        "axes[1, 1].axvline(x=0.81, color='r', linestyle='--', alpha=0.5, label='Default')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 5 configurations:\")\n",
        "print(top_10.head())"
      ],
      "metadata": {
        "id": "lRto0VUtea0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary comparison\n",
        "models_comparison = {\n",
        "    'Model': [\n",
        "        'Logistic Regression',\n",
        "        'Decision Tree',\n",
        "        'Random Forest (Default)',\n",
        "        'Random Forest (Tuned)'\n",
        "    ],\n",
        "    'Accuracy': [0.7900, 0.8045, 0.8100, 0.8384],\n",
        "    'Method': ['Baseline', 'Single Tree', 'Ensemble', 'Optimized Ensemble']\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(models_comparison)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(comparison_df['Model'], comparison_df['Accuracy'],\n",
        "               color=['blue', 'orange', 'green', 'red'])\n",
        "plt.ylim([0.75, 0.85])\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Titanic Survival Prediction - Model Evolution')\n",
        "plt.xticks(rotation=15, ha='right')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.2%}',\n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# Highlight best\n",
        "plt.axhline(y=0.8384, color='red', linestyle='--', alpha=0.3, label='Best Model')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPLETE MODEL COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nWINNER: Random Forest (Tuned) - {0.8384:.2%}\")\n",
        "print(f\"Total improvement from baseline: {(0.8384-0.79)*100:.2f}%\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "id": "hU_2xgVnfQMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning - Optimizing Random Forest\n",
        "\n",
        "## Problem\n",
        "Can we improve Random Forest performance beyond default settings?\n",
        "\n",
        "## Approach: Grid Search Cross-Validation\n",
        "\n",
        "Tested 108 hyperparameter combinations:\n",
        "- n_estimators: [50, 100, 200]\n",
        "- max_depth: [5, 10, 15, None]\n",
        "- min_samples_split: [2, 5, 10]\n",
        "- min_samples_leaf: [1, 2, 4]\n",
        "\n",
        "## Optimal Hyperparameters Found\n",
        "```python\n",
        "{\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 10,\n",
        "    'min_samples_split': 5,\n",
        "    'min_samples_leaf': 1\n",
        "}\n",
        "```\n",
        "\n",
        "## Results\n",
        "\n",
        "**Before Tuning:** 81.00% accuracy\n",
        "**After Tuning:** 83.84% accuracy\n",
        "**Improvement:** +2.84%\n",
        "\n",
        "## Model Evolution Summary\n",
        "\n",
        "| Model | Accuracy | Notes |\n",
        "|-------|----------|-------|\n",
        "| Logistic Regression | 79.00% | Baseline |\n",
        "| Decision Tree | 80.45% | Single tree |\n",
        "| Random Forest (Default) | 81.00% | Ensemble |\n",
        "| **Random Forest (Tuned)** | **83.84%** | **Optimized âœ“** |\n",
        "\n",
        "## Key Learnings\n",
        "\n",
        "1. **max_depth=10 prevents overfitting** - Unlimited depth performed worse\n",
        "2. **100 trees is sufficient** - More trees didn't help\n",
        "3. **Grid Search found 2.84% improvement** - Worth the computational cost\n",
        "4. **Systematic tuning beats guessing** - Tested all combinations\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "Hyperparameter tuning provided meaningful accuracy improvement. The optimized Random Forest model achieves 83.84% accuracy, correctly predicting survival for ~747 out of 891 passengers."
      ],
      "metadata": {
        "id": "LHGS35YDf69C"
      }
    }
  ]
}